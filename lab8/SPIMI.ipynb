{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "path_root = \"C:/BCSpace/study/ThisSemester/IR/lab/lab5\"\n",
    "dir_source = \"20_newsgroups\"\n",
    "dir_temp = \"_temp\"  # folder name for the processed files\n",
    "file_indexer = 'indexer.csv'    # new file to store the indexer\n",
    "\n",
    "path_source = os.path.join(path_root, dir_source)\n",
    "path_temp = os.path.join(path_root, dir_temp)\n",
    "doc_id = {}\n",
    "doc_list = []\n",
    "term_idf = {}\n",
    "spimi_index = {}\n",
    "\n",
    "stop_words = {'i', 'me', 'my', 'myself', 'we', \n",
    "              'our', 'ours', 'ourselves', 'you', \n",
    "              \"you're\", \"you've\", \"you'll\", \"you'd\", \n",
    "              'your', 'yours', 'yourself', 'yourselves', \n",
    "              'he', 'him', 'his', 'himself', 'she', \n",
    "              \"she's\", 'her', 'hers', 'herself', 'it', \n",
    "              \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', \n",
    "              'which', 'who', 'whom', 'this', 'that', \n",
    "              \"that'll\", 'these', 'those', 'am', 'is', \n",
    "              'are', 'was', 'were', 'be', 'been', \n",
    "              'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "              'the', 'and', 'but', 'if', 'or', 'because', \n",
    "              'as', 'until', 'while', 'of', 'at', 'by', \n",
    "              'for', 'with', 'about', 'against', 'between', \n",
    "              'into', 'through', 'during', 'before', 'after', \n",
    "              'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', \n",
    "              'again', 'further', 'then', 'once', 'here', \n",
    "              'there', 'when', 'where', 'why', 'how', \n",
    "              'all', 'any', 'both', 'each', 'few',\n",
    "              'more', 'most', 'other', 'some', 'such', \n",
    "              'no', 'nor', 'not', 'only', 'own', \n",
    "              'same', 'so', 'than', 'too', 'very', \n",
    "              's', 't', 'can', 'will', 'just', 'don', \n",
    "              \"don't\", 'should', \"should've\", 'now', 'd', \n",
    "              'll', 'm', 'o', 're', 've', 'y', 'ain', \n",
    "              'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n",
    "              'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", \n",
    "              'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "              \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "              'wouldn', \"wouldn't\"}\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some functions\n",
    "import csv\n",
    "from math import log10\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "import sys\n",
    "\n",
    "\n",
    "# Recursively finds size of objects\n",
    "def get_size(obj, seen=None):\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "\n",
    "# tokenize\n",
    "\n",
    "def get_term_freq(text):\n",
    "    text = text.lower()\n",
    "    # word_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    word_tokenizer = RegexpTokenizer('[A-Za-z]+')\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    term_set = set(words).difference(stop_words)\n",
    "    term_freq = {}\n",
    "    for w in words:\n",
    "        if w in term_set:\n",
    "            if w in term_freq:\n",
    "                term_freq[w] = term_freq[w] + 1\n",
    "            else:\n",
    "                term_freq[w] = 1\n",
    "    return term_freq\n",
    "\n",
    "\n",
    "# store sub dictionary in .csv\n",
    "def write_batch(cid, index):\n",
    "    batch_file = os.path.join(path_temp, str(cid) + '.csv')\n",
    "    with open(batch_file, mode='w+', newline='', encoding='utf-8') as f:\n",
    "        f_writer = csv.writer(f)\n",
    "        for key, value in index.items():\n",
    "            value.sort()\n",
    "            temp_list = [(key, len(value))] + value\n",
    "            f_writer.writerow(temp_list)\n",
    "          \n",
    "  \n",
    "def write_indexer(p, index):\n",
    "    with open(p, mode='w+', newline='', encoding='utf-8') as f:\n",
    "        f_writer = csv.writer(f)\n",
    "        for key, value in index.items():\n",
    "            temp_list = [(key, term_idf[key])] + value\n",
    "            f_writer.writerow(temp_list)\n",
    "\n",
    "\n",
    "def read_batch(p):\n",
    "    temp_index = {}\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        f_reader = csv.reader(f)\n",
    "        for row in f_reader:\n",
    "            if len(row) > 1:\n",
    "                rlst = []\n",
    "                for tu in row:\n",
    "                    rlst.append(tuple(eval(tu)))\n",
    "                temp_index[rlst[0][0]] = rlst[1:]\n",
    "    return temp_index \n",
    "    \n",
    "\n",
    "def read_indexer(p):\n",
    "    temp_index = {}\n",
    "    temp_idf = {}\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        f_reader = csv.reader(f)\n",
    "        for row in f_reader:\n",
    "            if len(row) > 1:\n",
    "                rlst = []\n",
    "                for tu in row:\n",
    "                    rlst.append(tuple(eval(tu)))\n",
    "                temp_index[rlst[0][0]] = rlst[1:]\n",
    "                temp_idf[rlst[0][0]] = rlst[0][1]\n",
    "    return temp_index, temp_idf\n",
    "\n",
    "\n",
    "def get_w(tf, idf):\n",
    "    if idf == 0 or tf == 0:\n",
    "        return 0\n",
    "    return (1 + log10(tf)) * log10(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the indexer and store into 'indexer.csv'\n",
    "\n",
    "# get the sub dictionaries\n",
    "from sys import getsizeof\n",
    "from math import sqrt\n",
    "\n",
    "chunk_size = 15000000\n",
    "\n",
    "\n",
    "def process_directions():\n",
    "    chunk_num = 0\n",
    "    term_index = {}\n",
    "    doc_list.clear()\n",
    "    doc_id.clear()\n",
    "    if not os.path.exists(path_temp):\n",
    "        os.mkdir(path_temp)\n",
    "    for root, dirs, files in os.walk(path_source):\n",
    "        for file_name in files:\n",
    "            fpn = os.path.join(root, file_name)\n",
    "            doc_list.append([fpn, 0])\n",
    "            file_cnt = len(doc_list) - 1\n",
    "            doc_id[fpn] = file_cnt\n",
    "            \n",
    "            with open(os.path.join(root, fpn), mode='r', errors='ignore', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            term_freq = get_term_freq(text)\n",
    "            for term, value in term_freq.items():\n",
    "                if term in term_index:\n",
    "                    term_index[term].append((file_cnt, value))\n",
    "                else:\n",
    "                    term_index[term] = [(file_cnt, value)]\n",
    "            if file_cnt % 100 == 0 and get_size(term_index) > chunk_size:\n",
    "                print(str(chunk_num) + '.csv' + ' created')\n",
    "                write_batch(chunk_num, term_index)\n",
    "                term_index.clear()\n",
    "                chunk_num = chunk_num + 1\n",
    "    if getsizeof(term_index) > 0:\n",
    "        print(str(chunk_num) + '.csv' + ' created')\n",
    "        write_batch(chunk_num, term_index)\n",
    "        chunk_num = chunk_num + 1\n",
    "    return chunk_num\n",
    "\n",
    "\n",
    "# merge index\n",
    "def merge_batch():\n",
    "    spimi_index.clear()\n",
    "    term_idf.clear()\n",
    "    for i in range(batch_num):\n",
    "        file = str(i) + '.csv'\n",
    "        file_name = os.path.join(path_temp, file)\n",
    "        sub_index = read_batch(file_name)\n",
    "        print(file + ' merged')\n",
    "        for key, value in sub_index.items():\n",
    "            if key in spimi_index:\n",
    "                spimi_index[key] = spimi_index[key] + value\n",
    "            else:\n",
    "                spimi_index[key] = value\n",
    "    file_num = len(doc_list)\n",
    "    for key, value in spimi_index.items():\n",
    "        term_idf[key] = float(file_num) / len(value)\n",
    "    write_indexer(os.path.join(path_root, file_indexer), spimi_index)\n",
    "\n",
    "\n",
    "def trans_w():\n",
    "    for key, value in spimi_index.items():\n",
    "        temp_list = value\n",
    "        for i in range(len(value)):\n",
    "            temp_list[i] = (temp_list[i][0], get_w(temp_list[i][1], term_idf[key]))\n",
    "            doc_list[temp_list[i][0]][1] = doc_list[temp_list[i][0]][1] + temp_list[i][1] * temp_list[i][1]\n",
    "        spimi_index[key] = temp_list\n",
    "    for i in range(len(doc_list)):\n",
    "        doc_list[i][1] = sqrt(doc_list[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.csv created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.csv merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.csv merged\n"
     ]
    }
   ],
   "source": [
    "# calculate the indexer\n",
    "batch_num = process_directions()\n",
    "merge_batch()\n",
    "\n",
    "trans_w()\n",
    "\n",
    "# Load indexer from file\n",
    "\n",
    "# spimi_index, term_idf = read_indexer(os.path.join(path_root, file_indexer))\n",
    "# trans_w()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\20_newsgroups\\comp.os.ms-windows.misc\\9981\n"
     ]
    }
   ],
   "source": [
    "# Query\n",
    "query_list = ['handwriting signature',\n",
    "              'Georgia Institute Technology',\n",
    "              'evidence piling',\n",
    "              'Information retrieval',\n",
    "              'Los Angeles',\n",
    "              'ugce'] \n",
    "query = query_list[5]\n",
    "\n",
    "query_freq = get_term_freq(query)\n",
    "candi_dict = {}\n",
    "\n",
    "for term, tfq in query_freq.items():\n",
    "    w = get_w(tfq, term_idf[term])\n",
    "    query_freq[term] = w\n",
    "\n",
    "for term in query_freq:\n",
    "    for fid, tf_idf in spimi_index[term]:\n",
    "        if fid in candi_dict:\n",
    "            temp_tuple = candi_dict[fid]\n",
    "            candi_dict[fid] = tf_idf * query_freq[term] + temp_tuple\n",
    "        else:\n",
    "            candi_dict[fid] = tf_idf * query_freq[term]\n",
    "\n",
    "candi_list = []\n",
    "for key, value in candi_dict.items():\n",
    "    candi_list.append((value / doc_list[key][1], key))\n",
    "\n",
    "candi_list.sort(reverse=True, key=lambda x: (x[0], -x[1]))\n",
    "\n",
    "for w, fid in candi_list[:10]:\n",
    "    print(doc_list[fid][0][len(path_root):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}