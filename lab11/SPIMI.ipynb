{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "import os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "path_root = \"C:/BCSpace/study/ThisSemester/IR/lab/lab5\"\n",
    "dir_source = \"20_newsgroups\"\n",
    "dir_temp = \"_temp\"  # folder name for the processed files\n",
    "file_indexer = 'indexer.csv'    # new file to store the indexer\n",
    "\n",
    "path_source = os.path.join(path_root, dir_source)\n",
    "path_temp = os.path.join(path_root, dir_temp)\n",
    "doc_id = {}\n",
    "doc_list = []\n",
    "doc_len = {}\n",
    "\n",
    "stop_words = {'i', 'me', 'my', 'myself', 'we', \n",
    "              'our', 'ours', 'ourselves', 'you', \n",
    "              \"you're\", \"you've\", \"you'll\", \"you'd\", \n",
    "              'your', 'yours', 'yourself', 'yourselves', \n",
    "              'he', 'him', 'his', 'himself', 'she', \n",
    "              \"she's\", 'her', 'hers', 'herself', 'it', \n",
    "              \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', \n",
    "              'which', 'who', 'whom', 'this', 'that', \n",
    "              \"that'll\", 'these', 'those', 'am', 'is', \n",
    "              'are', 'was', 'were', 'be', 'been', \n",
    "              'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "              'the', 'and', 'but', 'if', 'or', 'because', \n",
    "              'as', 'until', 'while', 'of', 'at', 'by', \n",
    "              'for', 'with', 'about', 'against', 'between', \n",
    "              'into', 'through', 'during', 'before', 'after', \n",
    "              'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', \n",
    "              'again', 'further', 'then', 'once', 'here', \n",
    "              'there', 'when', 'where', 'why', 'how', \n",
    "              'all', 'any', 'both', 'each', 'few',\n",
    "              'more', 'most', 'other', 'some', 'such', \n",
    "              'no', 'nor', 'not', 'only', 'own', \n",
    "              'same', 'so', 'than', 'too', 'very', \n",
    "              's', 't', 'can', 'will', 'just', 'don', \n",
    "              \"don't\", 'should', \"should've\", 'now', 'd', \n",
    "              'll', 'm', 'o', 're', 've', 'y', 'ain', \n",
    "              'aren', \"aren't\", 'couldn', \"couldn't\", \n",
    "              'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", \n",
    "              'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", \n",
    "              'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "              \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", \n",
    "              'wouldn', \"wouldn't\"}\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some functions\n",
    "import csv\n",
    "from math import log10\n",
    "\n",
    "from nltk import RegexpTokenizer\n",
    "import sys\n",
    "\n",
    "\n",
    "# Recursively finds size of objects\n",
    "def get_size(obj, seen=None):\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "    return size\n",
    "\n",
    "\n",
    "# tokenize\n",
    "\n",
    "def get_term_freq(text):\n",
    "    text = text.lower()\n",
    "    # word_tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    word_tokenizer = RegexpTokenizer('[A-Za-z]+')\n",
    "    words = word_tokenizer.tokenize(text)\n",
    "    term_set = set(words).difference(stop_words)\n",
    "    term_freq = {}\n",
    "    for w in words:\n",
    "        if w in term_set:\n",
    "            if w in term_freq:\n",
    "                term_freq[w] = term_freq[w] + 1\n",
    "            else:\n",
    "                term_freq[w] = 1\n",
    "    return term_freq, len(words)\n",
    "\n",
    "\n",
    "# store sub dictionary in .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-9703ff8a7fbd>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mterm_index\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_directions\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_temp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'w+'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnewline\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m''\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[0mf_writer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcsv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwriter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-7-9703ff8a7fbd>\u001B[0m in \u001B[0;36mprocess_directions\u001B[1;34m()\u001B[0m\n\u001B[0;32m     24\u001B[0m             \u001B[0mdoc_id\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfpn\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfile_cnt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m             \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfpn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'r'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'ignore'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m                 \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m             \u001B[0mterm_freq\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0md_len\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_term_freq\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Program Files\\Anaconda3\\lib\\codecs.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, errors)\u001B[0m\n\u001B[0;32m    307\u001B[0m     \u001B[0mbyte\u001B[0m \u001B[0msequences\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m     \"\"\"\n\u001B[1;32m--> 309\u001B[1;33m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'strict'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    310\u001B[0m         \u001B[0mIncrementalDecoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m         \u001B[1;31m# undecoded input that is kept between calls to decode()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the indexer and store into 'indexer.csv'\n",
    "\n",
    "# get the sub dictionaries\n",
    "from sys import getsizeof\n",
    "from math import sqrt\n",
    "\n",
    "chunk_size = 15000000\n",
    "\n",
    "\n",
    "def process_directions():\n",
    "    chunk_num = 0\n",
    "    term_index = {}\n",
    "    doc_list.clear()\n",
    "    doc_id.clear()\n",
    "    doc_len.clear()\n",
    "    if not os.path.exists(path_temp):\n",
    "        os.mkdir(path_temp)\n",
    "    for root, dirs, files in os.walk(path_source):\n",
    "        for file_name in files:\n",
    "            fpn = os.path.join(root, file_name)\n",
    "            doc_list.append(fpn)\n",
    "            file_cnt = len(doc_list) - 1\n",
    "            doc_id[fpn] = file_cnt\n",
    "\n",
    "            with open(fpn, mode='r', errors='ignore', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            term_freq, d_len = get_term_freq(text)\n",
    "            doc_len[file_cnt] = d_len\n",
    "            for term, value in term_freq.items():\n",
    "                if term in term_index:\n",
    "                    term_index[term].append((file_cnt, value))\n",
    "                else:\n",
    "                    term_index[term] = [(file_cnt, value)]\n",
    "    return term_index\n",
    "\n",
    "indexer = process_directions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china: 94 265 274 474 746 749 816 829 976 1659 3090 3711 3712 3713 3714 3896 5319 6410 8219 10120 11430 12730 12965 13458 13567 13645 13726 14012 14042 14556 15295 15446 16009 16010 16045 16177 16628 16766 16788 17007 17008 17036 17237 17444 17446 17502 17656 17677 17716 17750 17902 18004 18016 18025 18026 18087 18145 18209 18313 18371 18388 18437 19019 19109 19171 19232 19568 19629 19666 \n",
      "japan: 443 474 487 1129 1138 1215 1216 1520 1647 1689 1691 1896 1925 1942 1958 2169 2820 3407 3417 4216 4241 4252 4311 5035 5319 5482 5561 5783 5897 6335 6411 6414 6443 7032 7075 7162 7456 7466 7488 7763 7767 7770 7858 7862 7879 7952 8074 8269 8492 8865 8892 8933 9173 9208 9259 9269 9275 9544 9752 9779 9870 9951 10080 10120 11030 11348 11443 11450 11654 11671 11704 11714 13186 13220 13645 13702 14005 14011 14012 14014 14015 14017 14022 14071 14072 14106 14172 14175 14185 14228 14230 14492 14504 14556 14602 14654 14665 14675 14978 15514 15555 15772 16316 16692 16781 16841 17178 17219 17324 17415 17438 17444 17453 17486 17547 17575 17642 17643 17750 17978 18022 18040 18074 18086 18095 18104 18132 18242 18344 18383 18441 18454 18476 18535 18550 18790 18796 18797 18938 18956 19155 19345 19747 \n",
      "russia: 249 273 829 904 1009 1558 1559 1925 6129 6153 6275 6287 6520 6570 6735 6736 10058 10073 10106 10120 10126 10265 10503 10514 10597 10866 10871 10923 10924 10935 10987 10999 11213 11469 11487 11491 11704 11736 13345 14012 14051 14080 14195 14370 14375 14377 14421 14438 14492 14504 14522 14556 14654 14694 14737 14957 14971 15556 15965 15994 16761 16942 17002 17024 17106 17110 17124 17138 17184 17196 17197 17237 17238 17252 17382 17541 17568 17572 17640 17645 17646 17647 17648 17659 17727 17764 17797 17799 17807 17825 17841 17843 17847 17882 17924 17935 17937 17940 17960 17981 17983 18009 18016 18022 18034 18040 18087 18092 18095 18104 18111 18155 18197 18313 18371 18799 18938 18969 19171 19747 19800 19816 \n",
      "[(14012, 11.357919106795908), (14556, 11.076038158046082), (10120, 10.057679807475953), (5319, 8.319611633225168), (18371, 8.178381962734575), (19171, 8.178381962734575), (14492, 8.063046679447599), (18104, 7.510870842784685), (14504, 7.489353654864978), (18040, 7.464168655244611)]\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def weighted_tf(tf, d_len, avg_len):\n",
    "    return 2 * tf / (0.25 + 0.75 * d_len / avg_len + tf)\n",
    "\n",
    "def idf(df, n):\n",
    "    if df == 0:\n",
    "        return 0\n",
    "    return log(n / df, 10)\n",
    "\n",
    "def hw3_bm25_11812420(query, doc_len = {}, dictionary = {}):\n",
    "    avg_doc_len = 0\n",
    "    doc_num = len(doc_len)\n",
    "    for dl in doc_len:\n",
    "        avg_doc_len = avg_doc_len + dl / doc_num\n",
    "\n",
    "    candidate = {}\n",
    "    for k in query:\n",
    "        df = len(dictionary[k])\n",
    "        for did, tf in dictionary[k]:\n",
    "            if did not in candidate.keys():\n",
    "                candidate[did] = 0\n",
    "            candidate[did] = candidate[did] + weighted_tf(tf, doc_len[did], avg_doc_len) * idf(df, doc_num)\n",
    "\n",
    "    candidate = sorted(candidate.items(), key= lambda x: (x[1]), reverse= True)\n",
    "    return candidate[:10]\n",
    "\n",
    "# Query\n",
    "# for v in indexer['russia']:\n",
    "#     print(doc_list[v[0]][len(path_root):], v[1])\n",
    "query = ['china', 'japan', 'russia']\n",
    "#\n",
    "answer = hw3_bm25_11812420(query, doc_len, indexer)\n",
    "print(answer)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}